{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884dfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from layers.dense import Dense\n",
    "from layers.conv2d import Conv2D\n",
    "from layers.ReLU import ReLU\n",
    "from layers.utils import AdamOptimizer, Sequential, compute_accuracy\n",
    "from layers.softmaxcrossentropyloss import SoftmaxCrossEntropyLoss\n",
    "from layers.maxpool2d import MaxPool2D\n",
    "from layers.flatten import Flatten\n",
    "from layers.dropout import Dropout\n",
    "from data.data_loader import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e940e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "all_train_accuracies = {}\n",
    "all_val_accuracies = {}\n",
    "training_times = {}\n",
    "confusion_matrices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17015b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256]  # Batch sizes to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650aadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 32] Epoch 1/5\n",
      "Loss (train): 0.5296, Acc (train): 0.8466, Loss (val): 0.3521, Acc (val): 0.8971, Epoch completed in 10.11 seconds.\n",
      "[Batch size: 32] Epoch 2/5\n",
      "Loss (train): 0.3626, Acc (train): 0.8915, Loss (val): 0.3383, Acc (val): 0.8996, Epoch completed in 3.79 seconds.\n",
      "[Batch size: 32] Epoch 3/5\n",
      "Loss (train): 0.3546, Acc (train): 0.8946, Loss (val): 0.3284, Acc (val): 0.9013, Epoch completed in 3.77 seconds.\n",
      "[Batch size: 32] Epoch 4/5\n",
      "Loss (train): 0.3401, Acc (train): 0.8996, Loss (val): 0.3233, Acc (val): 0.9039, Epoch completed in 3.81 seconds.\n",
      "[Batch size: 32] Epoch 5/5\n",
      "Loss (train): 0.3368, Acc (train): 0.9000, Loss (val): 0.3309, Acc (val): 0.9029, Epoch completed in 3.80 seconds.\n",
      "[Batch size: 32] Training completed in 25.29 seconds.\n",
      "Wyniki zapisane do [32]custom_cnn_results.json\n",
      "[64] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 64] Epoch 1/5\n",
      "Loss (train): 0.5102, Acc (train): 0.8540, Loss (val): 0.3282, Acc (val): 0.9028, Epoch completed in 3.82 seconds.\n",
      "[Batch size: 64] Epoch 2/5\n",
      "Loss (train): 0.3372, Acc (train): 0.8989, Loss (val): 0.3103, Acc (val): 0.9090, Epoch completed in 3.81 seconds.\n",
      "[Batch size: 64] Epoch 3/5\n",
      "Loss (train): 0.3020, Acc (train): 0.9107, Loss (val): 0.2622, Acc (val): 0.9249, Epoch completed in 3.95 seconds.\n",
      "[Batch size: 64] Epoch 4/5\n",
      "Loss (train): 0.2505, Acc (train): 0.9257, Loss (val): 0.2218, Acc (val): 0.9354, Epoch completed in 3.87 seconds.\n",
      "[Batch size: 64] Epoch 5/5\n",
      "Loss (train): 0.2185, Acc (train): 0.9342, Loss (val): 0.2073, Acc (val): 0.9356, Epoch completed in 3.85 seconds.\n",
      "[Batch size: 64] Training completed in 19.30 seconds.\n",
      "Wyniki zapisane do [64]custom_cnn_results.json\n",
      "[128] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 128] Epoch 1/5\n",
      "Loss (train): 0.5614, Acc (train): 0.8367, Loss (val): 0.3582, Acc (val): 0.8898, Epoch completed in 3.83 seconds.\n",
      "[Batch size: 128] Epoch 2/5\n",
      "Loss (train): 0.3621, Acc (train): 0.8907, Loss (val): 0.3387, Acc (val): 0.8993, Epoch completed in 3.84 seconds.\n",
      "[Batch size: 128] Epoch 3/5\n",
      "Loss (train): 0.3420, Acc (train): 0.8978, Loss (val): 0.3194, Acc (val): 0.9081, Epoch completed in 3.82 seconds.\n",
      "[Batch size: 128] Epoch 4/5\n",
      "Loss (train): 0.3283, Acc (train): 0.9021, Loss (val): 0.3177, Acc (val): 0.9073, Epoch completed in 3.80 seconds.\n",
      "[Batch size: 128] Epoch 5/5\n",
      "Loss (train): 0.3183, Acc (train): 0.9060, Loss (val): 0.3067, Acc (val): 0.9111, Epoch completed in 3.81 seconds.\n",
      "[Batch size: 128] Training completed in 19.11 seconds.\n",
      "Wyniki zapisane do [128]custom_cnn_results.json\n",
      "[256] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 256] Epoch 1/5\n",
      "Loss (train): 0.5190, Acc (train): 0.8545, Loss (val): 0.2703, Acc (val): 0.9203, Epoch completed in 3.81 seconds.\n",
      "[Batch size: 256] Epoch 2/5\n",
      "Loss (train): 0.2602, Acc (train): 0.9224, Loss (val): 0.2328, Acc (val): 0.9329, Epoch completed in 3.82 seconds.\n",
      "[Batch size: 256] Epoch 3/5\n",
      "Loss (train): 0.2347, Acc (train): 0.9295, Loss (val): 0.2269, Acc (val): 0.9337, Epoch completed in 3.83 seconds.\n",
      "[Batch size: 256] Epoch 4/5\n",
      "Loss (train): 0.2252, Acc (train): 0.9319, Loss (val): 0.2118, Acc (val): 0.9360, Epoch completed in 3.81 seconds.\n",
      "[Batch size: 256] Epoch 5/5\n",
      "Loss (train): 0.2196, Acc (train): 0.9337, Loss (val): 0.2121, Acc (val): 0.9364, Epoch completed in 3.80 seconds.\n",
      "[Batch size: 256] Training completed in 19.07 seconds.\n",
      "Wyniki zapisane do [256]custom_cnn_results.json\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    # load data\n",
    "    print(f\"[{batch_size}] Loading data...\")\n",
    "    train_loader = Data(\n",
    "        path=\"D:/studia/SieciNeuronowe/dataset/train\",\n",
    "        batch_size=batch_size,\n",
    "        use_cupy=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = Data(\n",
    "        path=\"D:/studia/SieciNeuronowe/dataset/test\",\n",
    "        batch_size=batch_size,\n",
    "        use_cupy=True,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    print(\"Data loaded successfully.\")\n",
    "    # MODEL\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Conv2D(\n",
    "                input_channels=1, output_channels=8, kernel_size=2, stride=1, padding=1\n",
    "            ),\n",
    "            ReLU(),\n",
    "            MaxPool2D(kernel_size=2, stride=2),\n",
    "            Dropout(0.3),  # Dropout layer with 40% dropout rate\n",
    "            Flatten(),\n",
    "            Dense(input_size=8 * 14 * 14, output_size=10),\n",
    "        ]\n",
    "    )\n",
    "    loss_fn = SoftmaxCrossEntropyLoss()\n",
    "    optimizer = AdamOptimizer(learning_rate=0.005)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    epochs_done_all = {}\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 5\n",
    "    epochs_done = 0\n",
    "    num_epochs = 5\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"[Batch size: {batch_size}] Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            logits = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(logits, y_batch)\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            model.update(optimizer)\n",
    "\n",
    "            train_loss += float(loss)\n",
    "            train_acc += float(compute_accuracy(logits, y_batch))\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss /= num_batches\n",
    "        train_acc /= num_batches\n",
    "\n",
    "        val_logits = model.forward(val_loader.X)\n",
    "        val_loss = loss_fn.forward(val_logits, val_loader.y)\n",
    "        val_acc = compute_accuracy(val_logits, val_loader.y)\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "                break\n",
    "        print(\n",
    "            f\"Loss (train): {train_loss:.4f}, Acc (train): {train_acc:.4f}, \"\n",
    "            f\"Loss (val): {float(val_loss):.4f}, Acc (val): {float(val_acc):.4f}, \"\n",
    "            f\"Epoch completed in {time.time() - epoch_start_time:.2f} seconds.\"\n",
    "        )\n",
    "        epochs_done += 1\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(float(val_loss))\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(float(val_acc))\n",
    "    print(\n",
    "        f\"[Batch size: {batch_size}] Training completed in {time.time() - start_time:.2f} seconds.\"\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    epochs_done_all[batch_size] = epochs_done\n",
    "    training_times[batch_size] = training_time\n",
    "    all_train_losses[batch_size] = train_losses\n",
    "    all_val_losses[batch_size] = val_losses\n",
    "    all_train_accuracies[batch_size] = train_accuracies\n",
    "    all_val_accuracies[batch_size] = val_accuracies\n",
    "    true_labels = val_loader.y.get()\n",
    "    val_logits = model.forward(val_loader.X)\n",
    "    val_preds = np.argmax(val_logits.get(), axis=1)\n",
    "\n",
    "    cm = confusion_matrix(true_labels, val_preds)\n",
    "    confusion_matrices[batch_size] = cm\n",
    "    results = {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"val_loss\": val_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"val_accuracy\": val_accuracies,\n",
    "        \"training_time_seconds\": training_time,\n",
    "        \"confusion_matrix\": cm.tolist(),  # Convert to list for JSON serialization\n",
    "        \"epochs_done\": epochs_done,\n",
    "    }\n",
    "\n",
    "    # Save results to JSON file\n",
    "    with open(f\"../DOCS/[{batch_size}]custom_cnn_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    print(f\"Results saved: [{batch_size}]custom_cnn_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

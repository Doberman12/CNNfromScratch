{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884dfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from layers.backend import xp\n",
    "from layers.dense import Dense\n",
    "from layers.conv2d import Conv2D\n",
    "from layers.ReLU import ReLU\n",
    "from layers.utils import im2col_conv, col2im_conv, im2col_pool, col2im_pool, AdamOptimizer, Sequential, compute_accuracy\n",
    "from layers.softmaxcrossentropyloss import SoftmaxCrossEntropyLoss\n",
    "from layers.maxpool2d import MaxPool2D\n",
    "from layers.flatten import Flatten\n",
    "from layers.dropout import Dropout\n",
    "from data.data_loader import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e940e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "all_train_accuracies = {}\n",
    "all_val_accuracies = {}\n",
    "training_times = {}\n",
    "confusion_matrices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17015b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256]  # Batch sizes to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3650aadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 32] Epoch 1/10\n",
      "Loss (train): 0.3512, Acc (train): 0.8954, Loss (val): 0.2510, Acc (val): 0.9238, Epoch completed in 6.93 seconds.\n",
      "[Batch size: 32] Epoch 2/10\n",
      "Loss (train): 0.2337, Acc (train): 0.9291, Loss (val): 0.2295, Acc (val): 0.9297, Epoch completed in 6.73 seconds.\n",
      "[Batch size: 32] Epoch 3/10\n",
      "Loss (train): 0.1978, Acc (train): 0.9401, Loss (val): 0.1784, Acc (val): 0.9460, Epoch completed in 6.71 seconds.\n",
      "[Batch size: 32] Epoch 4/10\n",
      "Loss (train): 0.1730, Acc (train): 0.9476, Loss (val): 0.1593, Acc (val): 0.9504, Epoch completed in 6.50 seconds.\n",
      "[Batch size: 32] Epoch 5/10\n",
      "Loss (train): 0.1531, Acc (train): 0.9532, Loss (val): 0.1371, Acc (val): 0.9583, Epoch completed in 6.16 seconds.\n",
      "[Batch size: 32] Epoch 6/10\n",
      "Loss (train): 0.1362, Acc (train): 0.9591, Loss (val): 0.1332, Acc (val): 0.9598, Epoch completed in 6.34 seconds.\n",
      "[Batch size: 32] Epoch 7/10\n",
      "Loss (train): 0.1213, Acc (train): 0.9637, Loss (val): 0.1183, Acc (val): 0.9644, Epoch completed in 5.87 seconds.\n",
      "[Batch size: 32] Epoch 8/10\n",
      "Loss (train): 0.1141, Acc (train): 0.9649, Loss (val): 0.1178, Acc (val): 0.9628, Epoch completed in 5.89 seconds.\n",
      "[Batch size: 32] Epoch 9/10\n",
      "Loss (train): 0.1063, Acc (train): 0.9670, Loss (val): 0.1048, Acc (val): 0.9651, Epoch completed in 5.97 seconds.\n",
      "[Batch size: 32] Epoch 10/10\n",
      "Loss (train): 0.1029, Acc (train): 0.9672, Loss (val): 0.1097, Acc (val): 0.9663, Epoch completed in 5.94 seconds.\n",
      "[Batch size: 32] Training completed in 63.03 seconds.\n",
      "Wyniki zapisane do [32]custom_cnn_results.json\n",
      "[64] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 64] Epoch 1/10\n",
      "Loss (train): 0.4201, Acc (train): 0.8758, Loss (val): 0.3158, Acc (val): 0.9065, Epoch completed in 6.14 seconds.\n",
      "[Batch size: 64] Epoch 2/10\n",
      "Loss (train): 0.2833, Acc (train): 0.9166, Loss (val): 0.2101, Acc (val): 0.9421, Epoch completed in 5.94 seconds.\n",
      "[Batch size: 64] Epoch 3/10\n",
      "Loss (train): 0.2009, Acc (train): 0.9400, Loss (val): 0.1750, Acc (val): 0.9455, Epoch completed in 6.03 seconds.\n",
      "[Batch size: 64] Epoch 4/10\n",
      "Loss (train): 0.1763, Acc (train): 0.9463, Loss (val): 0.1658, Acc (val): 0.9520, Epoch completed in 6.03 seconds.\n",
      "[Batch size: 64] Epoch 5/10\n",
      "Loss (train): 0.1626, Acc (train): 0.9504, Loss (val): 0.1542, Acc (val): 0.9538, Epoch completed in 6.07 seconds.\n",
      "[Batch size: 64] Epoch 6/10\n",
      "Loss (train): 0.1532, Acc (train): 0.9532, Loss (val): 0.1523, Acc (val): 0.9520, Epoch completed in 5.98 seconds.\n",
      "[Batch size: 64] Epoch 7/10\n",
      "Loss (train): 0.1434, Acc (train): 0.9554, Loss (val): 0.1412, Acc (val): 0.9574, Epoch completed in 5.97 seconds.\n",
      "[Batch size: 64] Epoch 8/10\n",
      "Loss (train): 0.1356, Acc (train): 0.9576, Loss (val): 0.1337, Acc (val): 0.9603, Epoch completed in 6.05 seconds.\n",
      "[Batch size: 64] Epoch 9/10\n",
      "Loss (train): 0.1270, Acc (train): 0.9608, Loss (val): 0.1311, Acc (val): 0.9589, Epoch completed in 6.02 seconds.\n",
      "[Batch size: 64] Epoch 10/10\n",
      "Loss (train): 0.1197, Acc (train): 0.9636, Loss (val): 0.1248, Acc (val): 0.9617, Epoch completed in 6.05 seconds.\n",
      "[Batch size: 64] Training completed in 60.27 seconds.\n",
      "Wyniki zapisane do [64]custom_cnn_results.json\n",
      "[128] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 128] Epoch 1/10\n",
      "Loss (train): 0.3509, Acc (train): 0.8963, Loss (val): 0.2252, Acc (val): 0.9319, Epoch completed in 6.07 seconds.\n",
      "[Batch size: 128] Epoch 2/10\n",
      "Loss (train): 0.2211, Acc (train): 0.9347, Loss (val): 0.1804, Acc (val): 0.9457, Epoch completed in 5.91 seconds.\n",
      "[Batch size: 128] Epoch 3/10\n",
      "Loss (train): 0.1878, Acc (train): 0.9435, Loss (val): 0.1690, Acc (val): 0.9488, Epoch completed in 5.94 seconds.\n",
      "[Batch size: 128] Epoch 4/10\n",
      "Loss (train): 0.1687, Acc (train): 0.9491, Loss (val): 0.1533, Acc (val): 0.9541, Epoch completed in 6.04 seconds.\n",
      "[Batch size: 128] Epoch 5/10\n",
      "Loss (train): 0.1528, Acc (train): 0.9533, Loss (val): 0.1433, Acc (val): 0.9585, Epoch completed in 6.28 seconds.\n",
      "[Batch size: 128] Epoch 6/10\n",
      "Loss (train): 0.1403, Acc (train): 0.9568, Loss (val): 0.1412, Acc (val): 0.9563, Epoch completed in 6.35 seconds.\n",
      "[Batch size: 128] Epoch 7/10\n",
      "Loss (train): 0.1315, Acc (train): 0.9590, Loss (val): 0.1303, Acc (val): 0.9599, Epoch completed in 6.04 seconds.\n",
      "[Batch size: 128] Epoch 8/10\n",
      "Loss (train): 0.1234, Acc (train): 0.9625, Loss (val): 0.1230, Acc (val): 0.9628, Epoch completed in 5.98 seconds.\n",
      "[Batch size: 128] Epoch 9/10\n",
      "Loss (train): 0.1143, Acc (train): 0.9643, Loss (val): 0.1175, Acc (val): 0.9634, Epoch completed in 6.20 seconds.\n",
      "[Batch size: 128] Epoch 10/10\n",
      "Loss (train): 0.1108, Acc (train): 0.9649, Loss (val): 0.1185, Acc (val): 0.9628, Epoch completed in 6.14 seconds.\n",
      "[Batch size: 128] Training completed in 60.94 seconds.\n",
      "Wyniki zapisane do [128]custom_cnn_results.json\n",
      "[256] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 256] Epoch 1/10\n",
      "Loss (train): 0.4380, Acc (train): 0.8675, Loss (val): 0.3307, Acc (val): 0.9008, Epoch completed in 6.11 seconds.\n",
      "[Batch size: 256] Epoch 2/10\n",
      "Loss (train): 0.3464, Acc (train): 0.8953, Loss (val): 0.3074, Acc (val): 0.9089, Epoch completed in 6.03 seconds.\n",
      "[Batch size: 256] Epoch 3/10\n",
      "Loss (train): 0.2886, Acc (train): 0.9142, Loss (val): 0.2401, Acc (val): 0.9313, Epoch completed in 5.99 seconds.\n",
      "[Batch size: 256] Epoch 4/10\n",
      "Loss (train): 0.2254, Acc (train): 0.9326, Loss (val): 0.1898, Acc (val): 0.9445, Epoch completed in 5.95 seconds.\n",
      "[Batch size: 256] Epoch 5/10\n",
      "Loss (train): 0.1856, Acc (train): 0.9439, Loss (val): 0.1784, Acc (val): 0.9449, Epoch completed in 5.96 seconds.\n",
      "[Batch size: 256] Epoch 6/10\n",
      "Loss (train): 0.1696, Acc (train): 0.9479, Loss (val): 0.1610, Acc (val): 0.9501, Epoch completed in 6.06 seconds.\n",
      "[Batch size: 256] Epoch 7/10\n",
      "Loss (train): 0.1613, Acc (train): 0.9502, Loss (val): 0.1683, Acc (val): 0.9458, Epoch completed in 6.27 seconds.\n",
      "[Batch size: 256] Epoch 8/10\n",
      "Loss (train): 0.1612, Acc (train): 0.9508, Loss (val): 0.1528, Acc (val): 0.9540, Epoch completed in 6.43 seconds.\n",
      "[Batch size: 256] Epoch 9/10\n",
      "Loss (train): 0.1575, Acc (train): 0.9513, Loss (val): 0.1638, Acc (val): 0.9492, Epoch completed in 6.08 seconds.\n",
      "[Batch size: 256] Epoch 10/10\n",
      "Loss (train): 0.1537, Acc (train): 0.9519, Loss (val): 0.1571, Acc (val): 0.9517, Epoch completed in 6.08 seconds.\n",
      "[Batch size: 256] Training completed in 60.97 seconds.\n",
      "Wyniki zapisane do [256]custom_cnn_results.json\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    # load data\n",
    "    print(f\"[{batch_size}] Loading data...\")\n",
    "    train_loader = Data(\n",
    "        path=\"D:/studia/SieciNeuronowe/dataset/train\", batch_size=64, use_cupy=True\n",
    "    )\n",
    "    val_loader = Data(\n",
    "        path=\"D:/studia/SieciNeuronowe/dataset/test\", batch_size=64, use_cupy=True\n",
    "    )\n",
    "    print(\"Data loaded successfully.\")\n",
    "    # MODEL\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Conv2D(\n",
    "                input_channels=1, output_channels=8, kernel_size=2, stride=1, padding=1\n",
    "            ),\n",
    "            ReLU(),\n",
    "            MaxPool2D(kernel_size=2, stride=2),\n",
    "            Dropout(0.3),  # Dropout layer with 40% dropout rate\n",
    "            Flatten(),\n",
    "            Dense(input_size=8 * 14 * 14, output_size=10),\n",
    "        ]\n",
    "    )\n",
    "    loss_fn = SoftmaxCrossEntropyLoss()\n",
    "    optimizer = AdamOptimizer(learning_rate=0.005)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    epochs_done_all = {}\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 5\n",
    "    epochs_done = 0\n",
    "    num_epochs = 10\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"[Batch size: {batch_size}] Epoch {epoch+1}/{num_epochs}\")\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            logits = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(logits, y_batch)\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            model.update(optimizer)\n",
    "\n",
    "            train_loss += float(loss)\n",
    "            train_acc += float(compute_accuracy(logits, y_batch))\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss /= num_batches\n",
    "        train_acc /= num_batches\n",
    "\n",
    "        val_logits = model.forward(val_loader.X)\n",
    "        val_loss = loss_fn.forward(val_logits, val_loader.y)\n",
    "        val_acc = compute_accuracy(val_logits, val_loader.y)\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "        print(\n",
    "            f\"Loss (train): {train_loss:.4f}, Acc (train): {train_acc:.4f}, \"\n",
    "            f\"Loss (val): {float(val_loss):.4f}, Acc (val): {float(val_acc):.4f}, \"\n",
    "            f\"Epoch completed in {time.time() - epoch_start_time:.2f} seconds.\"\n",
    "        )\n",
    "        epochs_done += 1\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(float(val_loss))\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(float(val_acc))\n",
    "    print(\n",
    "        f\"[Batch size: {batch_size}] Training completed in {time.time() - start_time:.2f} seconds.\"\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    epochs_done_all[batch_size] = epochs_done\n",
    "    training_times[batch_size] = training_time\n",
    "    all_train_losses[batch_size] = train_losses\n",
    "    all_val_losses[batch_size] = val_losses\n",
    "    all_train_accuracies[batch_size] = train_accuracies\n",
    "    all_val_accuracies[batch_size] = val_accuracies\n",
    "    true_labels = val_loader.y.get()\n",
    "    val_logits = model.forward(val_loader.X)\n",
    "    val_preds = np.argmax(val_logits.get(), axis=1)\n",
    "\n",
    "    cm = confusion_matrix(true_labels, val_preds)\n",
    "    confusion_matrices[batch_size] = cm\n",
    "    results = {\n",
    "        \"train_loss\": train_losses,\n",
    "        \n",
    "        \"val_loss\": val_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"val_accuracy\": val_accuracies,\n",
    "        \"training_time_seconds\": training_time,\n",
    "        \"confusion_matrix\": cm.tolist(),  # Convert to list for JSON serialization\n",
    "        \"epochs_done\": epochs_done,\n",
    "    }\n",
    "\n",
    "    # Save results to JSON file\n",
    "    with open(f\"../DOCS/[{batch_size}]custom_cnn_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    print(f\"Wyniki zapisane do [{batch_size}]custom_cnn_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

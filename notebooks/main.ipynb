{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884dfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from layers.dense import Dense\n",
    "from layers.conv2d import Conv2D\n",
    "from layers.ReLU import ReLU\n",
    "from layers.utils import im2col, col2im, AdamOptimizer, Sequential, compute_accuracy\n",
    "from layers.softmaxcrossentropyloss import SoftmaxCrossEntropyLoss\n",
    "from layers.maxpool2d import MaxPool2D\n",
    "from layers.flatten import Flatten\n",
    "from layers.dropout import Dropout\n",
    "from data.data_loader import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e940e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_losses = {}\n",
    "all_val_losses = {}\n",
    "all_train_accuracies = {}\n",
    "all_val_accuracies = {}\n",
    "training_times = {}\n",
    "confusion_matrices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17015b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256]  # Batch sizes to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650aadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32] Loading data...\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/train\\cached_data_cupy.npz\n",
      "Loading cached dataset from D:/studia/SieciNeuronowe/dataset/test\\cached_data_cupy.npz\n",
      "Data loaded successfully.\n",
      "[Batch size: 32] Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    # load data\n",
    "    print(f\"[{batch_size}] Loading data...\")\n",
    "    train_loader = Data(\n",
    "        path=\"D:/studia/SieciNeuronowe/dataset/train\", batch_size=64, use_cupy=True\n",
    "    )\n",
    "    val_loader = Data(\n",
    "        path=\"D:/studia/SieciNeuronowe/dataset/test\", batch_size=64, use_cupy=True\n",
    "    )\n",
    "    print(\"Data loaded successfully.\")\n",
    "    # MODEL\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Conv2D(\n",
    "                input_channels=1, output_channels=8, kernel_size=2, stride=1, padding=1\n",
    "            ),\n",
    "            ReLU(),\n",
    "            MaxPool2D(kernel_size=2, stride=2),\n",
    "            Dropout(0.3),  # Dropout layer with 40% dropout rate\n",
    "            Flatten(),\n",
    "            Dense(input_size=8 * 14 * 14, output_size=10),\n",
    "        ]\n",
    "    )\n",
    "    loss_fn = SoftmaxCrossEntropyLoss()\n",
    "    optimizer = AdamOptimizer(learning_rate=0.005)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    epochs_done_all = {}\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 5\n",
    "    epochs_done = 0\n",
    "    num_epochs = 50\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"[Batch size: {batch_size}] Epoch {epoch+1}/{num_epochs}\")\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            logits = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(logits, y_batch)\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            model.update(optimizer)\n",
    "\n",
    "            train_loss += float(loss)\n",
    "            train_acc += float(compute_accuracy(logits, y_batch))\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss /= num_batches\n",
    "        train_acc /= num_batches\n",
    "\n",
    "        val_logits = model.forward(val_loader.X)\n",
    "        val_loss = loss_fn.forward(val_logits, val_loader.y)\n",
    "        val_acc = compute_accuracy(val_logits, val_loader.y)\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "        print(\n",
    "            f\"Loss (train): {train_loss:.4f}, Acc (train): {train_acc:.4f}, \"\n",
    "            f\"Loss (val): {float(val_loss):.4f}, Acc (val): {float(val_acc):.4f}, \"\n",
    "            f\"Epoch completed in {time.time() - epoch_start_time:.2f} seconds.\"\n",
    "        )\n",
    "        epochs_done += 1\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(float(val_loss))\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(float(val_acc))\n",
    "    print(\n",
    "        f\"[Batch size: {batch_size}] Training completed in {time.time() - start_time:.2f} seconds.\"\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    epochs_done_all[batch_size] = epochs_done\n",
    "    training_times[batch_size] = training_time\n",
    "    all_train_losses[batch_size] = train_losses\n",
    "    all_val_losses[batch_size] = val_losses\n",
    "    all_train_accuracies[batch_size] = train_accuracies\n",
    "    all_val_accuracies[batch_size] = val_accuracies\n",
    "    true_labels = val_loader.y.get()\n",
    "    val_logits = model.forward(val_loader.X)\n",
    "    val_preds = np.argmax(val_logits.get(), axis=1)\n",
    "\n",
    "    cm = confusion_matrix(true_labels, val_preds)\n",
    "    confusion_matrices[batch_size] = cm\n",
    "    results = {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"val_loss\": val_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"val_accuracy\": val_accuracies,\n",
    "        \"training_time_seconds\": training_time,\n",
    "        \"confusion_matrix\": cm.tolist(),  # Convert to list for JSON serialization\n",
    "        \"epochs_done\": epochs_done,\n",
    "    }\n",
    "\n",
    "    # Save results to JSON file\n",
    "    with open(f\"../DOCS/[{batch_size}]custom_cnn_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    print(f\"Wyniki zapisane do [{batch_size}]custom_cnn_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
